{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2795202,"sourceType":"datasetVersion","datasetId":575905},{"sourceId":6731076,"sourceType":"datasetVersion","datasetId":3877014},{"sourceId":7594172,"sourceType":"datasetVersion","datasetId":4420264}],"dockerImageVersionId":30674,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#Import Libraries\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.metrics import accuracy_score, precision_score, f1_score, classification_report\nimport pickle\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.utils import shuffle\nfrom nltk.corpus import stopwords\nimport matplotlib.pyplot as plt\nfrom sklearn import metrics\nimport pandas as pd\nfrom wordcloud import WordCloud\nimport numpy as np\nimport torch\nfrom torch.utils.data import DataLoader, TensorDataset\nimport string\nimport itertools\nimport nltk\nimport re\nimport wandb\nfrom wandb.keras import WandbCallback\nimport os\n# Set a random seed for reproducibility\nnp.random.seed(42)\n\nfrom kaggle_secrets import UserSecretsClient\n\nuser_secrets = UserSecretsClient()\n\nmy_secret = user_secrets.get_secret(\"wandb_api_key\") \n\nwandb.login(key=my_secret)","metadata":{"execution":{"iopub.status.busy":"2024-05-21T13:45:24.482895Z","iopub.execute_input":"2024-05-21T13:45:24.483311Z","iopub.status.idle":"2024-05-21T13:45:49.457291Z","shell.execute_reply.started":"2024-05-21T13:45:24.483273Z","shell.execute_reply":"2024-05-21T13:45:49.456058Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2024-05-21 13:45:34.930806: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-21 13:45:34.930941: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-21 13:45:35.071483: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"#Importing Datasets\ndata = pd.read_csv(\"/kaggle/input/abstracts/research-abstracts-labeled.csv\")\nTest_data = pd.read_csv(\"/kaggle/input/gpt-vs-human-a-corpus-of-research-abstracts/data_set.csv\", low_memory=False)\n#Checking if Dataset has any null Values\ndata.isnull().sum()\n\ndata = shuffle(data)\ndata = data.reset_index( drop=True )\n\n#Removed duplicate datat\ndata.drop_duplicates(subset=['text'])\n#Removed unnecessay columns\n#data.drop([\"title\",\"word_count\"],axis=1,inplace=True)\ndata.dropna()\ndata.describe()\n\ndata.head()\n\ndata.to_csv(\"sub.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2024-05-21T13:45:52.690830Z","iopub.execute_input":"2024-05-21T13:45:52.692362Z","iopub.status.idle":"2024-05-21T13:45:55.242507Z","shell.execute_reply.started":"2024-05-21T13:45:52.692323Z","shell.execute_reply":"2024-05-21T13:45:55.241540Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"columns_to_keep = ['title','abstract','is_ai_generated']\nTest_data = Test_data[columns_to_keep]\nTest_data.head()\n\n#Test Data Cleaning\nTest_data.isnull().sum()\nTest_data = Test_data.dropna()\n\n#data Cleaning\nnltk.download('stopwords')\nstop =  stopwords.words('english')\n\ndef remove_punctuation(text):\n    translator = str.maketrans(\"\", \"\", string.punctuation)\n    return text.translate(translator)\ndata['text'] = data['text'].apply(remove_punctuation)\ndata['text'] = data['text'].str.replace('\\n','')\ndata['text'] = data['text'].str.replace('\\d+', '', regex=True)\ndata['text'] = data['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\ndata['text'] = data['text'].str.lower()\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-05T18:15:53.330506Z","iopub.execute_input":"2024-05-05T18:15:53.331199Z","iopub.status.idle":"2024-05-05T18:16:01.585094Z","shell.execute_reply.started":"2024-05-05T18:15:53.331165Z","shell.execute_reply":"2024-05-05T18:16:01.584119Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Roberta Model\nimport random\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch import nn, optim\nfrom transformers import RobertaTokenizer, RobertaModel\nimport tensorflow_text as text\n\ntokenizer = RobertaTokenizer.from_pretrained(\"/kaggle/input/roberta-base\")\n\ndef tokenize_and_truncate(text, max_length=512):\n    if isinstance(text, float):\n        text = str(int(text))  # Convert float to integer and then to string\n    # Tokenize and truncate\n    tokens = tokenizer.tokenize(text)[:max_length]  # leaving space for special tokens\n    # Add the special tokens\n    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n    \n    return token_ids\n\n#Adding Tokens to both Datasets\ndata[\"tokens\"] = data[\"text\"].apply(lambda x: tokenize_and_truncate(x))\nTest_data[\"tokens\"] = Test_data[\"abstract\"].apply(lambda x: tokenize_and_truncate(x))","metadata":{"execution":{"iopub.status.busy":"2024-05-05T18:16:18.560277Z","iopub.execute_input":"2024-05-05T18:16:18.561098Z","iopub.status.idle":"2024-05-05T18:17:03.935546Z","shell.execute_reply.started":"2024-05-05T18:16:18.561058Z","shell.execute_reply":"2024-05-05T18:17:03.934539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nclass RobertaForBinaryClassification(nn.Module):\n    def __init__(self):\n        super(RobertaForBinaryClassification, self).__init__()\n\n        self.roberta = RobertaModel.from_pretrained(\"/kaggle/input/roberta-base\")\n        hidden_size = self.roberta.config.hidden_size\n\n        self.fc1 = nn.Linear(hidden_size, hidden_size)\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(0.1)\n        self.fc2 = nn.Linear(hidden_size, 1)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, input_ids, attention_mask=None):\n        outputs = self.roberta(input_ids, attention_mask=attention_mask)\n        cls_output = outputs.last_hidden_state[:, 0, :]\n\n        x = self.fc1(cls_output)\n        x = self.relu(x)\n        x = self.dropout(x)\n        logits = self.fc2(x)\n        probs = self.sigmoid(logits)\n        return probs\n    \nclass TextDataset(Dataset):\n    def __init__(self, token_lists, labels, max_token_len=512):\n        self.token_lists = token_lists\n        self.labels = labels\n        self.max_token_len = max_token_len\n\n    def __len__(self):\n        return len(self.token_lists)\n\n    def __getitem__(self, idx):\n        # Pad the token lists to the max length\n        token_list = self.token_lists[idx][:self.max_token_len]  # truncate if longer than max length\n        padded_tokens = token_list + [0] * (self.max_token_len - len(token_list))  # pad with zeros\n        attention_mask = [1 if i < len(token_list) else 0 for i in range(self.max_token_len)]\n        return torch.tensor(padded_tokens), torch.tensor(attention_mask), torch.tensor(self.labels[idx])","metadata":{"execution":{"iopub.status.busy":"2024-05-05T18:17:14.365504Z","iopub.execute_input":"2024-05-05T18:17:14.365861Z","iopub.status.idle":"2024-05-05T18:17:14.427660Z","shell.execute_reply.started":"2024-05-05T18:17:14.365834Z","shell.execute_reply":"2024-05-05T18:17:14.426487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CFG = dict(\n    optimiser = 'Adam',\n    learning_rate = 1e-6,\n)\n\nepoch = [1,2,3,4,5]","metadata":{"execution":{"iopub.status.busy":"2024-05-05T18:17:29.569415Z","iopub.execute_input":"2024-05-05T18:17:29.569752Z","iopub.status.idle":"2024-05-05T18:17:29.574403Z","shell.execute_reply.started":"2024-05-05T18:17:29.569725Z","shell.execute_reply":"2024-05-05T18:17:29.573438Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Splitting the dataframe to have train and testing data\ntrain_data, test_data = train_test_split(data, test_size=0.3, random_state = 42)","metadata":{"execution":{"iopub.status.busy":"2024-05-05T18:17:33.035956Z","iopub.execute_input":"2024-05-05T18:17:33.036580Z","iopub.status.idle":"2024-05-05T18:17:33.049658Z","shell.execute_reply.started":"2024-05-05T18:17:33.036553Z","shell.execute_reply":"2024-05-05T18:17:33.048498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import recall_score\nfrom wandb.keras import WandbCallback\nfrom tqdm.auto import tqdm\n# Create Dataset and DataLoader\n\n#Training\ntrain_dataset = TextDataset(train_data[\"tokens\"].tolist(), train_data[\"label\"].tolist())\nbatch_size = 8\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n\n#Tetsing\ntest_dataset = TextDataset(test_data[\"tokens\"].tolist(), test_data[\"label\"].tolist())\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n\n# Instantiate the model\nmodel = RobertaForBinaryClassification()\nmodel = nn.DataParallel(model)\nmodel.to(device)\n# Define loss function and optimizer\nloss_function = nn.BCELoss()\noptimizer = optim.Adam(model.parameters(), lr=CFG['learning_rate'])","metadata":{"execution":{"iopub.status.busy":"2024-05-05T18:17:35.370359Z","iopub.execute_input":"2024-05-05T18:17:35.370986Z","iopub.status.idle":"2024-05-05T18:17:40.724403Z","shell.execute_reply.started":"2024-05-05T18:17:35.370948Z","shell.execute_reply":"2024-05-05T18:17:40.723586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lists to store metrics for each epoch\ntrain_losses = []\ntrain_accuracies = []\nval_losses = []\nval_accuracies = []\n\n#Training\nfrom sklearn.metrics import recall_score\nfrom wandb.keras import WandbCallback\nfrom tqdm.auto import tqdm\nfor x in epoch:\n    #Initialise Run\n    run = wandb.init(name = \"ROBERTA_Hyperparameter\",\n                    project = \"Dissertation_FinalResults\",\n                    config = CFG,)\n    model.train()\n    train_loss = 0\n    total_correct = 0\n    total_samples = 0\n\n    # Initialize lists to store true labels and predictions\n    all_predictions = []\n    all_labels = []\n\n    for tokens, attention_masks, labels in tqdm(train_loader):\n        tokens, attention_masks, labels = tokens.to(device), attention_masks.to(device), labels.to(device)  # Move to GPU if available\n        optimizer.zero_grad()\n        labels = labels.float().unsqueeze(1)  # Adjust labels' shape if necessary\n\n        outputs = model(tokens, attention_mask=attention_masks)\n        loss = loss_function(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        train_loss += loss.item()\n\n        # Run inference\n        outputs = model(tokens, attention_mask=attention_masks)\n        # Convert outputs to predictions (binary classification)\n        predictions = (outputs > 0.5).int() \n        total_correct += (predictions == labels).sum().item()\n        total_samples += labels.size(0)\n        all_predictions.extend(predictions.cpu().numpy())\n        all_labels.extend(labels.cpu().numpy())\n    \n     # Calculate average training loss and accuracy for the epoch\n    avg_train_loss = train_loss / len(train_loader)\n    train_accuracy = total_correct / total_samples\n    \n    \n    # Flatten the lists and convert to numpy arrays\n    all_predictions = np.array(all_predictions).flatten()\n    all_labels = np.array(all_labels).flatten()\n\n    # Calculate metrics\n    accuracy = accuracy_score(all_labels, all_predictions)\n    precision = precision_score(all_labels, all_predictions)\n    recall = recall_score(all_labels, all_predictions)\n    f1 = f1_score(all_labels, all_predictions)\n    \n     # Validation Phase\n    model.eval()\n    val_loss = 0\n    total_correct = 0\n    total_samples = 0\n\n    with torch.no_grad():\n        for tokens, attention_masks, labels in tqdm(test_loader):\n            tokens, attention_masks, labels = tokens.to(device), attention_masks.to(device), labels.to(device)\n            labels = labels.float().unsqueeze(1)\n\n            outputs = model(tokens, attention_mask=attention_masks)\n            loss = loss_function(outputs, labels)\n            val_loss += loss.item()\n\n            # Compute accuracy\n            predictions = (outputs > 0.5).int()\n            total_correct += (predictions == labels).sum().item()\n            total_samples += labels.size(0)\n\n    # Calculate average validation loss and accuracy for the epoch\n    avg_val_loss = val_loss / len(test_loader)\n    val_accuracy = total_correct / total_samples\n\n    # Store metrics for plotting and logging\n    train_losses.append(avg_train_loss)\n    train_accuracies.append(train_accuracy)\n    val_losses.append(avg_val_loss)\n    val_accuracies.append(val_accuracy) \n    \n        \n    wandb.log({\n            \"Epoch\":x,\n            \"Accuracy\": accuracy,\n            \"Precision\": precision,\n            \"F1 Score\": f1,\n            \"Recall\": recall,\n            \"Train Loss\": avg_train_loss,\n            \"Train Accuracy\": train_accuracy,\n            \"Val Loss\": avg_val_loss,\n            \"Val Accuracy\": val_accuracy\n        })\n    # Complete W&B run\n    run.finish()     ","metadata":{"execution":{"iopub.status.busy":"2024-05-05T18:22:05.635324Z","iopub.execute_input":"2024-05-05T18:22:05.635757Z","iopub.status.idle":"2024-05-05T20:17:22.965628Z","shell.execute_reply.started":"2024-05-05T18:22:05.635725Z","shell.execute_reply":"2024-05-05T20:17:22.964727Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Train_Labels = pd.DataFrame({\n    'Original Labels' : all_labels,\n    'Predicted Labels': all_predictions\n})\nTrain_Labels","metadata":{"execution":{"iopub.status.busy":"2024-05-05T20:31:09.573222Z","iopub.execute_input":"2024-05-05T20:31:09.574071Z","iopub.status.idle":"2024-05-05T20:31:09.587166Z","shell.execute_reply.started":"2024-05-05T20:31:09.574036Z","shell.execute_reply":"2024-05-05T20:31:09.586313Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Train_Labels.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2024-05-05T20:31:39.616506Z","iopub.execute_input":"2024-05-05T20:31:39.617211Z","iopub.status.idle":"2024-05-05T20:31:39.646773Z","shell.execute_reply.started":"2024-05-05T20:31:39.617179Z","shell.execute_reply":"2024-05-05T20:31:39.645874Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nimport seaborn as sns\ncm = confusion_matrix(all_labels,all_predictions)\n\nsns.heatmap(cm, \n            annot=True,\n            fmt='g', \n            xticklabels=['Machine','Human'],\n            yticklabels=['Machine','Human'])\nplt.ylabel('Prediction',fontsize=13)\nplt.xlabel('Actual',fontsize=13)\nplt.title('Confusion Matrix',fontsize=17)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-05-05T20:38:05.903245Z","iopub.execute_input":"2024-05-05T20:38:05.903610Z","iopub.status.idle":"2024-05-05T20:38:06.214299Z","shell.execute_reply.started":"2024-05-05T20:38:05.903582Z","shell.execute_reply":"2024-05-05T20:38:06.213415Z"},"trusted":true},"execution_count":null,"outputs":[]}]}