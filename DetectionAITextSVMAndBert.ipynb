{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2795202,"sourceType":"datasetVersion","datasetId":575905},{"sourceId":6731076,"sourceType":"datasetVersion","datasetId":3877014},{"sourceId":7594172,"sourceType":"datasetVersion","datasetId":4420264},{"sourceId":2644,"sourceType":"modelInstanceVersion","modelInstanceId":1910},{"sourceId":2938,"sourceType":"modelInstanceVersion","modelInstanceId":2180}],"dockerImageVersionId":30646,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#Import Libraries\n!pip install wandb\nimport wandb\nfrom sklearn.metrics import accuracy_score, precision_score, f1_score, classification_report\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.utils import shuffle\nfrom nltk.corpus import stopwords\nimport matplotlib.pyplot as plt\nfrom sklearn import metrics\nimport pandas as pd\nfrom wordcloud import WordCloud\nimport numpy as np\nimport torch\nfrom torch.utils.data import DataLoader, TensorDataset\nimport string\nimport itertools\nimport nltk\nimport re\nimport tensorflow as tf\nfrom tensorflow import keras\nimport tensorflow_text as text  # Registers the ops.\nimport tensorflow_hub as hub\nimport os\n\n# Set a random seed for reproducibility\nnp.random.seed(42)\n\nfrom kaggle_secrets import UserSecretsClient\n\nuser_secrets = UserSecretsClient()\n\nmy_secret = user_secrets.get_secret(\"wandb_api_key\") \n\nwandb.login(key=my_secret)","metadata":{"execution":{"iopub.status.busy":"2024-05-21T09:56:31.415155Z","iopub.execute_input":"2024-05-21T09:56:31.415953Z","iopub.status.idle":"2024-05-21T09:57:04.405923Z","shell.execute_reply.started":"2024-05-21T09:56:31.415922Z","shell.execute_reply":"2024-05-21T09:57:04.405013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Importing Datasets\ndata = pd.read_csv(\"/kaggle/input/abstracts/research-abstracts-labeled.csv\")\nTest_data = pd.read_csv(\"/kaggle/input/gpt-vs-human-a-corpus-of-research-abstracts/data_set.csv\", low_memory=False)\n#Checking if Dataset has any null Values\ndata.isnull().sum()\n\n\ndata = shuffle(data)\ndata = data.reset_index( drop=True )\n\n#Removed duplicate data\ndata.drop_duplicates(subset=['text'])\ndata.dropna()\ndata.describe()\n\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-21T09:57:11.794333Z","iopub.execute_input":"2024-05-21T09:57:11.795617Z","iopub.status.idle":"2024-05-21T09:57:12.551794Z","shell.execute_reply.started":"2024-05-21T09:57:11.795577Z","shell.execute_reply":"2024-05-21T09:57:12.550794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#removing unnecessary columns\ncolumns_to_keep = ['title','abstract','is_ai_generated']\nTest_data = Test_data[columns_to_keep]\n#Test Data Cleaning\nTest_data.isnull().sum()\nTest_data = Test_data.dropna()\n\nTest_data","metadata":{"execution":{"iopub.status.busy":"2024-05-21T09:57:19.748343Z","iopub.execute_input":"2024-05-21T09:57:19.749465Z","iopub.status.idle":"2024-05-21T09:57:19.768824Z","shell.execute_reply.started":"2024-05-21T09:57:19.749418Z","shell.execute_reply":"2024-05-21T09:57:19.767821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Parts of Speech Code\nHumanDataset = data[data['label'] == 0]\nHumanDataset = HumanDataset.sample()\nAIDataset = data[data['label'] == 1]\nAIDataset = AIDataset.sample()\nimport spacy\nimport seaborn as sns\n\n# Load spaCy model\nnlp = spacy.load(\"en_core_web_sm\")\n\n# Function to perform POS tagging\ndef pos_tagging(text):\n    doc = nlp(text)\n    return [(token.text, token.pos_) for token in doc]\n\n# Apply POS tagging to each dataframe\nHumanDataset['pos_tags'] = HumanDataset['text'].apply(pos_tagging)\nAIDataset['pos_tags'] = AIDataset['text'].apply(pos_tagging)\n\n# Flatten the list of POS tags in each data frame\npos_list_human = [pos for sublist in HumanDataset['pos_tags'] for _, pos in sublist]\npos_list_AI = [pos for sublist in AIDataset['pos_tags'] for _, pos in sublist]\n\n# Create a frequency distribution of POS tags\npos_freq_human = pd.Series(pos_list_human).value_counts()\npos_freq_AI = pd.Series(pos_list_AI).value_counts()\n\n# Combine the frequency distributions into a single dataframe\npos_freq_combined = pd.DataFrame({\n    'Human': pos_freq_human,\n    'AI': pos_freq_AI\n}).fillna(0)\n\n# Reset index for plotting\npos_freq_combined = pos_freq_combined.reset_index().melt(id_vars='index', var_name='Dataset', value_name='Frequency')\npos_freq_combined.columns = ['POS Tag', 'Dataset', 'Frequency']\n\n# Sorting the DataFrame by frequency within each Dataset\npos_freq_combined['Frequency'] = pos_freq_combined.groupby('Dataset')['Frequency'].transform(lambda x: x.sort_values(ascending=False).values)\n\n# Plotting the POS tag frequencies\nplt.figure(figsize=(14, 7))\nsns.barplot(x='POS Tag', y='Frequency', hue='Dataset', data=pos_freq_combined, palette='viridis')\nplt.title('Part of Speech Tag Distribution: Human vs AI')\nplt.xlabel('POS Tag')\nplt.ylabel('Frequency')\nplt.xticks(rotation=45)\nplt.legend(title='Dataset')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-05-21T09:57:23.018074Z","iopub.execute_input":"2024-05-21T09:57:23.018561Z","iopub.status.idle":"2024-05-21T09:57:28.140248Z","shell.execute_reply.started":"2024-05-21T09:57:23.018522Z","shell.execute_reply":"2024-05-21T09:57:28.139220Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#data Cleaning\nnltk.download('stopwords')\nstop =  stopwords.words('english')\n\ndef remove_punctuation(text):\n    translator = str.maketrans(\"\", \"\", string.punctuation)\n    return text.translate(translator)\ndata['text'] = data['text'].apply(remove_punctuation)\ndata['text'] = data['text'].str.replace('\\n','')\ndata['text'] = data['text'].str.replace('\\d+', '', regex=True)\ndata['text'] = data['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\ndata['text'] = data['text'].str.lower()\ndata.head()\n","metadata":{"execution":{"iopub.status.busy":"2024-05-15T14:51:26.999055Z","iopub.execute_input":"2024-05-15T14:51:26.999662Z","iopub.status.idle":"2024-05-15T14:51:36.379748Z","shell.execute_reply.started":"2024-05-15T14:51:26.999609Z","shell.execute_reply":"2024-05-15T14:51:36.378612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Visuzlise the Data\nlabel_count = data.label.value_counts()\nplt.pie(label_count,labels= label_count)\nplt.title('Labels')\nplt.legend(label_count.keys().tolist())\n\nvar_data = data.label\nfig = plt.figure(figsize=(10,4))\nplt.boxplot(var_data)\nplt.title('data distribution')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#word clouds\nall_words = ' '.join([text for text in data['text'][data['label'] == 1] ])\nwordcloud = WordCloud(width=800, height=500, random_state=42, max_font_size=110).generate(all_words)\nplt.figure(figsize=(10, 7))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()\n\nall_words = ' '.join([text for text in data['text'][data['label'] == 0] ])\nwordcloud = WordCloud(width=800, height=500, random_state=42, max_font_size=110).generate(all_words)\nplt.figure(figsize=(10, 7))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#show word length distribution\nimport seaborn as sns\ng = sns.FacetGrid(data,col='label', col_wrap = 2)\ng.map(plt.hist,'word_count')\ng.set_axis_labels(\"Word Length\")\nplt.show()\n\n#remove unnecessary columns from the data\ndata.drop([\"title\",\"word_count\"],axis=1,inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = data['text'].values\ny = data['label'].values\n\n# Using TF-IDF vectorization to convert text data into numerical features\ntfidf_vectorizer = TfidfVectorizer(max_features=1500)\nX_tfidf = tfidf_vectorizer.fit_transform(X).toarray()\n\n# Split the data into training and testing sets for SVM\nX_train_tfidf, X_test_tfidf, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.3,random_state=42)\n#Mention the amount of in Training and in Testing\nprint(f'Training {X_train_tfidf.shape[0]}\\nTesting {X_test_tfidf.shape[0]}')","metadata":{"execution":{"iopub.status.busy":"2024-05-15T14:51:51.349496Z","iopub.execute_input":"2024-05-15T14:51:51.349942Z","iopub.status.idle":"2024-05-15T14:51:55.076174Z","shell.execute_reply.started":"2024-05-15T14:51:51.349907Z","shell.execute_reply":"2024-05-15T14:51:55.075033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#SVM hyper parameter tuning\nparam_grid = {'C': [0.1, 1, 10],\n              'gamma': [1, 0.1, 0.01],\n              'kernel': ['rbf','linear']}\nsvm = SVC()\n\ngrid = RandomizedSearchCV(estimator = svm,\n                           param_distributions = param_grid,\n                           cv=5)\n\n#Fitting the model\ngrid.fit(X_train_tfidf,y_train)\nprint(f'Best Paramters:  {grid.best_estimator_}')","metadata":{"execution":{"iopub.status.busy":"2024-05-15T14:52:30.657484Z","iopub.execute_input":"2024-05-15T14:52:30.657895Z","iopub.status.idle":"2024-05-15T15:01:45.437519Z","shell.execute_reply.started":"2024-05-15T14:52:30.657867Z","shell.execute_reply":"2024-05-15T15:01:45.436303Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Predictions\npredictions = grid.best_estimator_.predict(X_test_tfidf)\naccuracy_score(y_test,predictions)","metadata":{"execution":{"iopub.status.busy":"2024-05-15T15:01:49.856168Z","iopub.execute_input":"2024-05-15T15:01:49.857025Z","iopub.status.idle":"2024-05-15T15:02:45.496273Z","shell.execute_reply.started":"2024-05-15T15:01:49.856988Z","shell.execute_reply":"2024-05-15T15:02:45.495121Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Classification report for the tuned model\nprint(classification_report(y_test,predictions))","metadata":{"execution":{"iopub.status.busy":"2024-05-15T15:03:50.614474Z","iopub.execute_input":"2024-05-15T15:03:50.614886Z","iopub.status.idle":"2024-05-15T15:03:50.638510Z","shell.execute_reply.started":"2024-05-15T15:03:50.614854Z","shell.execute_reply":"2024-05-15T15:03:50.637713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nimport seaborn as sns\n#creation of the confusion matrix with the y_test and the prediction value generated \ncm = metrics.confusion_matrix(y_test, predictions)\nsns.heatmap(cm, \n            annot=True,\n            fmt='g', \n            xticklabels=['Machine','Human'],\n            yticklabels=['Machine','Human'])\nplt.ylabel('Prediction',fontsize=13)\nplt.xlabel('Actual',fontsize=13)\nplt.title('Confusion Matrix',fontsize=17)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-05-15T15:03:53.827880Z","iopub.execute_input":"2024-05-15T15:03:53.828546Z","iopub.status.idle":"2024-05-15T15:03:54.294533Z","shell.execute_reply.started":"2024-05-15T15:03:53.828514Z","shell.execute_reply":"2024-05-15T15:03:54.292974Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#dictionary for the weights and baised information logging\nCFG = dict(\n    optimiser = 'Adam',\n    learning_rate = 1e-6,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#BERT Model\n# Function to create the BERT model\ndef bert_model():\n    model_path =\"/kaggle/input/bert/tensorflow2/bert-en-uncased-l-12-h-768-a-12/2\"\n    preprocess_path = \"/kaggle/input/bert/tensorflow2/en-uncased-preprocess/3/\"\n    \n    # Input layer for text data\n    text_input = tf.keras.layers.Input(shape=(), dtype=tf.string)\n    \n      # Preprocessor layer to convert text input to BERT-compatible format\n    preprocessor = hub.KerasLayer(preprocess_path)\n    encoder_inputs  = preprocessor(text_input) # preprocessed text\n    input_ids = encoder_inputs['input_word_ids']\n    attention_mask = encoder_inputs['input_mask']\n    # Convert labels to tensor format \n    labels = torch.tensor(data['label'].values)\n    \n    # Encoder layer using the BERT model\n    encoder = hub.KerasLayer(model_path,trainable=True)\n    outputs = encoder(encoder_inputs)\n    \n    # Extract pooled output and sequence output from the encoder\n    pooled_output = outputs['pooled_output'] # [batch_size, 512].\n    sequence_output = outputs[\"sequence_output\"] # [batch_size, seq_length, 512].\n    \n    # Add dropout and dense layers for classification\n    dropout = tf.keras.layers.Dropout(0.51 , name=\"dropout1\")(pooled_output)\n    dense_2 = tf.keras.layers.Dense(64 , activation='relu')(dropout)\n    dropout = tf.keras.layers.Dropout(0.3 , name=\"dropout2\")(dense_2)\n\n    dense_out = tf.keras.layers.Dense(1 , activation='sigmoid', name='output')(dropout)\n\n    model = tf.keras.Model(inputs=text_input, outputs=dense_out)\n    model.summary()\n    \n    # Compile the model with optimizer, loss function, and evaluation metrics\n    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=CFG['learning_rate']),\n              loss='binary_crossentropy',\n              metrics=[\"acc\"])\n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import recall_score\nfrom wandb.keras import WandbCallback\nmodel = bert_model()\n\n#Lists for Hyperparameter Tuning\nepochs = [1,2,3,4,5]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Splitting the Data\nX_train,X_test,y_train,y_test = train_test_split(data['text'], np.ravel(data.label), test_size=0.3,random_state=42)\n\n#Mention the amount of in Training and in Testing\nprint(f'Training {X_train.shape[0]}\\nTesting {X_test.shape[0]}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for x in epochs:\n   # Initialise a new run in Weights and Biases (W&B) for tracking this experiment\n    run = wandb.init(name = \"BERT_Hyperparameter\",\n               project = \"Dissertation_FinalResults\",\n               config = CFG,\n    )\n    # Train the model with the current number of epochs\n    history = model.fit(X_train, y_train , \n                        batch_size=8, \n                        callbacks=[WandbCallback()],\n                        verbose = True,\n                        epochs=x , \n                        validation_data=(X_test, y_test)\n                       )\n\n    loss_train , acc_train = model.evaluate(X_train, y_train)\n    print(\"Acc. Train data using epoch\",x,\":\",acc_train)\n    loss_test , acc_test = model.evaluate(X_test, y_test)\n    print(\"Acc. Test data using epoch\",x,\":\",acc_test)\n    \n    # Make predictions on the test data\n    y_pred = model.predict(X_test)\n    \n    # Calculate performance metrics\n    accuracy = accuracy_score(y_test,y_pred.round())\n    precision = precision_score(y_test,y_pred.round(),average='weighted')\n    f1 = f1_score(y_test,y_pred.round(),average='weighted')\n    recall = recall_score(y_test, y_pred.round(), average='weighted')\n\n    # Log the metrics to W&B\n    wandb.log({\n        \"Epoch\":x,\n        \"Train Loss\":loss_train,\n        \"Train Accuracy\": acc_train,\n        \"Accuracy\": accuracy,\n        \"Precision\": precision,\n        \"F1 Score\": f1,\n        \"Recall\": recall,\n        \"Val Loss\": loss_test,\n        \"Val Accuracy\": acc_test\n    })\n    # Complete W&B run\n    run.finish()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Convert the predicitions to either 0 or 1\ny_pred = np.array(y_pred.round()).flatten()\ny_pred","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#predictions on the testing data\nTest_Labels = pd.DataFrame({\n    'Original Labels' : y_test,\n    'Predicted Labels': y_pred.round()\n})\nTest_Labels","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Export file to CSV\nTest_Labels.to_csv(\"test_predict_labels.csv\", index=False)\npd.read_csv(\"/kaggle/working/test_predict_labels.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Confusion matrix\nimport seaborn as sns\ncm = metrics.confusion_matrix(y_test,y_pred.round())\n\nsns.heatmap(cm, \n            annot=True,\n            fmt='g', \n            xticklabels=['Machine','Human'],\n            yticklabels=['Machine','Human'])\nplt.ylabel('Prediction',fontsize=13)\nplt.xlabel('Actual',fontsize=13)\nplt.title('Confusion Matrix',fontsize=17)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Prediction on Test Data the model has never seen\ntest_data_pred = model.predict(Test_data['abstract'])\ntest_data_pred = np.array(test_data_pred).flatten()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data_pred = test_data_pred.round()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Saving the output in a CSV file\nsubmission = pd.DataFrame({'abstract': Test_data['abstract'] ,'original Label':Test_data['is_ai_generated'] ,'generated Percentage': test_data_pred})\nsubmission.to_csv('submissionBERT.csv', index=False)  # Save the CSV file\npd.read_csv(\"/kaggle/working/submissionBERT.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}